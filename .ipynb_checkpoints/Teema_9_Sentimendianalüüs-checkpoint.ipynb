{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentimendianalüüs\n",
    "\n",
    "\n",
    "## Sisukord\n",
    "\n",
    "* [Ülesanne 9 ja Boonusülesanne](#9)\n",
    "* [Andmete lugemine](#lugemine)\n",
    "* [Sõnade relevantsuse hindamine](#relevants)\n",
    "* [Logistiline regressioon dokumentide klassifitseerimiseks](#klf)\n",
    "\n",
    "Põhineb S.Raschka *Python Machine Learnig* raamatu \n",
    "[peatükil 8](https://github.com/rasbt/python-machine-learning-book/blob/master/code/ch08) ([*MIT litsents*](https://github.com/rasbt/python-machine-learning-book/blob/master/LICENSE.txt))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='9'></a>\n",
    "\n",
    "\n",
    "## Ülesanne 9.BOONUS\n",
    "\n",
    "Koostada oma originaalandmetest (vt ka Moodle sektsiooni *Iseseisev töö*) .csv fail. Kirjeldada andmestikku lühidalt ja dokumenteerida andmestiku atribuudid eraldi failis kujul **atribuudi_nimi, andmetüüp (tõeväärtus, kategooria, täisarv, reaalarv, tekst, kuupäev,...), selgitus**. Kui on olemas klass või arvväärtus, mida soovime ennustada, siis dokumenteerida ka see.\n",
    "\n",
    "Viige oma originaalandmestik sellisele kujule, mida ka teised tudengid saaksid kasutada ja laadige see Moodlesse üles. Vaadake, et teil oleksid seaduslikud  ja tööalased õigused neid andmeid jagada. Anonümiseerige isikuandmed (kui on) st asendage nimed ja isikukoodid meelevaldsete koodidega.\n",
    "\n",
    "See läheb kirja ka kui ülesande 9. esitamine, mida lisaks pole vaja teha. \n",
    "\n",
    "Kuigi iseseisvas töös on lubatud kasutada avalikke andmeid ka [Kagglest](https://www.kaggle.com/datasets) ja muudest allikatest, siis selle ülesandena lähevad kirja ainult andmed, mis pole avalikult saadaval või mis on saadud avalikest andmetest töötlemise abil. Näiteks Kagglest võetud .csv faili muutmata kujul esitamine ei ole selle boonusülesande mõte.\n",
    "\n",
    "\n",
    "## Ülesanne 9\n",
    "\n",
    "Kui originaalandmeid ei leia:\n",
    "\n",
    "Laadida Project Gutenberg lehelt alla Goethe Fausti http://www.gutenberg.org/ebooks/14591 ja Iliase http://www.gutenberg.org/ebooks/6130 tekstifailid (*plain text, utf-8*). Jagada need failid salmideks, kus salme eraldavad kaks reavahetust ja salmipikkus on üle saja ja alla tuhande tähemärgi, näiteks:\n",
    "\n",
    "`text = file.read()\n",
    "salmid = [t for t in text.split(\"\\n\\n\") if 100 < len(t) < 1000]`\n",
    "\n",
    "Koostage .csv fail, kus read on salmid ja esimeseks veeruks on salmi tekst ning teiseks veeruks on salmi allikas (Ilias või Faust)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_csv = []\n",
    "with open('./faust.txt') as f:\n",
    "    text = f.read().replace(',', '')\n",
    "    out_csv += [(v.replace('\\n', ''), 'faust') for v in text.split('\\n\\n') if 100 < len(v) < 1000]\n",
    "    \n",
    "with open('./ilias.txt') as f:\n",
    "    text = f.read().replace(',', '')\n",
    "    out_csv += [(v.replace('\\n', ''), 'ilias') for v in text.split('\\n\\n') if 100 < len(v) < 1000]\n",
    "\n",
    "with open('./verses.csv', 'w+') as f:\n",
    "    f.write('\\n'.join([','.join(x) for x in out_csv]))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='lugemine'></a>\n",
    "## Andmete lugemine\n",
    "\n",
    "Andmete töötluseks ja movie_data.csv faili koostamiseks on eraldi ipynb märkmik."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I thought this was one of those really great f...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PLEASE people! DO NOT bother with this poorly ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I must tell you right up front, I am certainly...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  I thought this was one of those really great f...          1\n",
       "1  PLEASE people! DO NOT bother with this poorly ...          0\n",
       "2  I must tell you right up front, I am certainly...          0"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Eeldus: movie_data.csv on tekitatud\n",
    "\n",
    "df = pd.read_csv('./movie_data.csv')\n",
    "df.head(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='relevants'></a>\n",
    "## Sõnade relevantsuse hindamine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Terminisagedus $tf(t, d)$ näitab kui mitu korda termin $t$ esineb dokumendis $d$. Klassi [CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) transformeerimismeetod `fit_transform(X)` teisendab tekstimassiivi $t \\times d$ terminisagedusmassiiviks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': 5, 'sun': 3, 'is': 1, 'shining': 2, 'weather': 6, 'sweet': 4, 'and': 0}\n",
      "[[0 1 1 1 0 1 0]\n",
      " [0 1 0 0 1 1 1]\n",
      " [1 2 1 1 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "count = CountVectorizer()\n",
    "docs = np.array([\n",
    "    'The sun is shining',\n",
    "    'The weather is sweet',\n",
    "    'The sun is shining and weather is sweet'])\n",
    "bag = count.fit_transform(docs)\n",
    "\n",
    "print(count.vocabulary_)\n",
    "print(bag.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sõnad, mis esinevad peaaegu kõigis dokumentides ei ole tavaliselt   dokumentide eristamise mõttes kuigi informatiivsed. Mõõt idf (*inverse document frequency*)  on logaritm dokumentide arvu $n_d$ suhtest sõna $t$ sisaldavate dokumentide arvu $f_d(t)$ ja on seega seda suurem, mida vähemates dokumentides sõna esineb. Vahel modiitseeritakse seda funktsiooni liites jagajale või jagatavale arvu 1.\n",
    "\n",
    "$$ idf (t) = log \\frac{n_d}{f_d(t)} $$\n",
    "\n",
    "Mõõt **tf-idf** (*term frequency - inverse document frequency*) korrigeerib terminisagedust $tf$ mõõduga $idf$\n",
    "\n",
    "$$ tfidf (t, d) = tf(i, d) \\times idf(t) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.1 , 0.  , 0.41, 0.41, 0.41, 0.  , 0.41])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = bag.toarray()\n",
    "\n",
    "def idf(X):\n",
    "    \"\"\" \n",
    "    Leiame idf  mõõtude vektori kõigile sõnadele kui meie \n",
    "    sagedusmaatriks (bag.toarray()) on X.\n",
    "    \"\"\"\n",
    "    n_d = len(X)\n",
    "    f_d = np.sum(X != 0, axis=0)\n",
    "    #print(f_d)\n",
    "    return np.log(n_d / f_d)\n",
    "\n",
    "idf(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.  , 0.  , 0.41, 0.41, 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  , 0.41, 0.  , 0.41],\n",
       "       [1.1 , 0.  , 0.41, 0.41, 0.41, 0.  , 0.41]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tfidf(X):\n",
    "    \"\"\" \n",
    "    Leiame tfidf  sageduste vektori kõigile sõnadele kui meie \n",
    "    sagedusmaatriks (bag.toarray()) on X.\n",
    "    \"\"\"\n",
    "    return X * idf(X)\n",
    "\n",
    "tfidf(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moodul sklearn pakub selleks klassi [TfidfTransformer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html). Tulemused erinevad seoses funktsioonide veidi  erinevate definitsioonidega \n",
    "\n",
    "$$ idf (t) = log \\frac{1+n_d}{1+f_d(t)} $$\n",
    "\n",
    "\n",
    "$$ tfidf (t, d) = tf(i, d) \\times (idf(t) + 1) $$\n",
    "\n",
    "ja dokumendi sagedusvektori normaliseerimisega ühikpikkuseks (L2-normaliseerimine)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.  , 0.43, 0.56, 0.56, 0.  , 0.43, 0.  ],\n",
       "       [0.  , 0.43, 0.  , 0.  , 0.56, 0.43, 0.56],\n",
       "       [0.44, 0.53, 0.34, 0.34, 0.34, 0.26, 0.34]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf_t = TfidfTransformer()\n",
    "TFIDF_X = tfidf_t.fit_transform(count.fit_transform(docs)).toarray()\n",
    "TFIDF_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1.])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(TFIDF_X**2, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='klf'></a>\n",
    "## Logistiline regressioon dokumentide klassifitseerimiseks\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Klass [TfidfVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) lihtsustab tf-idf mõõdu leidmist olles samaväärne  [CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) ja [TfidfTransformer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html) üksteise järel rakendamisega. Alustuseks leiame hüperparameetrite otsinguga [GrisSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.htm) parimad parameetrid. Kasutame selleks ainult 5000 objekti, muidu oleks see samm väga ajamahukas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df.loc[:5000, 'review'].values\n",
    "y_train = df.loc[:5000, 'sentiment'].values\n",
    "X_test = df.loc[5000:10000, 'review'].values\n",
    "y_test = df.loc[5000:10000, 'sentiment'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'kkloooo@'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(ord(\".\"))\n",
    "\"k,k,l,o,ooo.\".translate({44: None, 46: \"@\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#{ord(c):None for c in string.punctuation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(strip_accents=None,\n",
    "                        lowercase=False,\n",
    "                        preprocessor=None)\n",
    "\n",
    "def tokenizer(text):return text.split()\n",
    "def nopunctuation_tokenizer(text):\n",
    "    np_text = text.translate({ord(c):None for c in string.punctuation})\n",
    "    return np_text.split()\n",
    "\n",
    "param_grid = [{'vect__ngram_range': [(1, 1)],\n",
    "               'vect__tokenizer': [tokenizer, nopunctuation_tokenizer],\n",
    "               'clf__penalty': ['l1', 'l2'],\n",
    "               'clf__C': [1.0, 10.0, 100.0]},\n",
    "              {'vect__ngram_range': [(1, 1)],\n",
    "               'vect__tokenizer': [tokenizer, nopunctuation_tokenizer],\n",
    "               'vect__use_idf':[False],\n",
    "               'vect__norm':[None],\n",
    "               'clf__penalty': ['l1', 'l2'],\n",
    "               'clf__C': [1.0, 10.0, 100.0]},\n",
    "              ]\n",
    "\n",
    "lr_tfidf = Pipeline([('vect', tfidf),\n",
    "                     ('clf', LogisticRegression(random_state=0, solver=\"liblinear\"))])\n",
    "\n",
    "gs_lr_tfidf = GridSearchCV(lr_tfidf, param_grid,\n",
    "                           scoring='accuracy',\n",
    "                           cv=5,\n",
    "                           verbose=1,\n",
    "                           n_jobs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 120 out of 120 | elapsed:  2.0min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score=nan,\n",
       "             estimator=Pipeline(memory=None,\n",
       "                                steps=[('vect',\n",
       "                                        TfidfVectorizer(analyzer='word',\n",
       "                                                        binary=False,\n",
       "                                                        decode_error='strict',\n",
       "                                                        dtype=<class 'numpy.float64'>,\n",
       "                                                        encoding='utf-8',\n",
       "                                                        input='content',\n",
       "                                                        lowercase=False,\n",
       "                                                        max_df=1.0,\n",
       "                                                        max_features=None,\n",
       "                                                        min_df=1,\n",
       "                                                        ngram_range=(1, 1),\n",
       "                                                        norm='l2',\n",
       "                                                        preprocessor=None,\n",
       "                                                        smooth_idf=True,\n",
       "                                                        stop_words=None,\n",
       "                                                        strip_acc...\n",
       "                                              <function nopunctuation_tokenizer at 0x000001E915D682F0>]},\n",
       "                         {'clf__C': [1.0, 10.0, 100.0],\n",
       "                          'clf__penalty': ['l1', 'l2'],\n",
       "                          'vect__ngram_range': [(1, 1)], 'vect__norm': [None],\n",
       "                          'vect__tokenizer': [<function tokenizer at 0x000001E915D5F378>,\n",
       "                                              <function nopunctuation_tokenizer at 0x000001E915D682F0>],\n",
       "                          'vect__use_idf': [False]}],\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring='accuracy', verbose=1)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Väga ajamahukas samm\n",
    "gs_lr_tfidf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parimad parameetrid:  {'clf__C': 100.0, 'clf__penalty': 'l2', 'vect__ngram_range': (1, 1), 'vect__tokenizer': <function nopunctuation_tokenizer at 0x000001E915D682F0>}\n",
      "\n",
      "Täpsus:  0.8582253746253746\n"
     ]
    }
   ],
   "source": [
    "print(\"Parimad parameetrid: \", gs_lr_tfidf.best_params_)\n",
    "print(\"\\nTäpsus: \", gs_lr_tfidf.best_score_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seejärel kasutame neid parameetreid ennustava mudeli treenimiseks andmestiku esimese 25 000 objekti peal ja leiame kõige suurema koefitsendiga (kõige kaalukamad) sõnad otsuse tegemisel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('vect',\n",
       "                 TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.float64'>,\n",
       "                                 encoding='utf-8', input='content',\n",
       "                                 lowercase=False, max_df=1.0, max_features=None,\n",
       "                                 min_df=1, ngram_range=(1, 1), norm='l2',\n",
       "                                 preprocessor=None, smooth_idf=True,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 sublinear_tf=False,\n",
       "                                 token_pattern='...\n",
       "                                 tokenizer=<function nopunctuation_tokenizer at 0x000001E915D682F0>,\n",
       "                                 use_idf=True, vocabulary=None)),\n",
       "                ('clf',\n",
       "                 LogisticRegression(C=100.0, class_weight=None, dual=False,\n",
       "                                    fit_intercept=True, intercept_scaling=1,\n",
       "                                    l1_ratio=None, max_iter=100,\n",
       "                                    multi_class='auto', n_jobs=None,\n",
       "                                    penalty='l2', random_state=0,\n",
       "                                    solver='liblinear', tol=0.0001, verbose=0,\n",
       "                                    warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = df.loc[:25000, 'review'].values\n",
    "y_train = df.loc[:25000, 'sentiment'].values\n",
    "X_test = df.loc[25000:, 'review'].values\n",
    "y_test = df.loc[25000:, 'sentiment'].values\n",
    "\n",
    "lr_tfidf.set_params(**gs_lr_tfidf.best_params_)\n",
    "lr_tfidf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teeme 10-kordse ristkontrolli."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV täpsused:  [0.89 0.89 0.88 0.88 0.9  0.88 0.9  0.89 0.9  0.9 ]\n",
      "CV keskmine täpsus: 0.892 +/- 0.007\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scores = cross_val_score(estimator=lr_tfidf,\n",
    "                         X=X_train,\n",
    "                         y=y_train,\n",
    "                         cv=10,\n",
    "                         n_jobs=1)\n",
    "print('CV täpsused: ', scores)\n",
    "print('CV keskmine täpsus: %.3f' % np.mean(scores), \"+/- %.3f\" % np.std(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ja lõpuks kontrollime erinevate meetrikate ja testandmetega."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Täpsus testandmetel: 0.89476\n",
      "F1-skoor testandmetel: 0.8951082406410716\n",
      "Eksimismaatriks:\n",
      "[[11143  1405]\n",
      " [ 1226 11226]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_pred = lr_tfidf.predict(X_test)\n",
    "print(\"Täpsus testandmetel:\", accuracy_score(y_test, y_pred))\n",
    "print(\"F1-skoor testandmetel:\", f1_score(y_test, y_pred))\n",
    "print(\"Eksimismaatriks:\")\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Koefitsendid: [-0.04 -4.86  1.09 ...  0.06 -0.01 -2.4 ]\n"
     ]
    }
   ],
   "source": [
    "coef = lr_tfidf.named_steps['clf'].coef_[0]\n",
    "print(\"Koefitsendid:\", coef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sõnastik: 145431\n"
     ]
    }
   ],
   "source": [
    "word_dict = lr_tfidf.named_steps['vect'].vocabulary_\n",
    "print(\"Sõnastik:\", len(word_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Need sõnad mõjutavad arvustuse sentimenti enim (koefitsent, sõna):\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(-30.163405140363256, 'worst'),\n",
       " (-23.484700985222013, 'awful'),\n",
       " (-21.420029416165136, 'waste'),\n",
       " (21.331065960953826, '710'),\n",
       " (18.210690229167227, 'excellent'),\n",
       " (-16.629842670432787, 'bad'),\n",
       " (-16.576548522894914, 'fails'),\n",
       " (-16.42485928512674, 'boring'),\n",
       " (16.272315235901335, 'great'),\n",
       " (-15.785860680556352, 'nothing'),\n",
       " (15.763725870990394, 'wonderful'),\n",
       " (-15.704029862739269, 'disappointing'),\n",
       " (-14.80804035948606, 'poorly'),\n",
       " (-14.598099772128512, 'poor'),\n",
       " (-14.326725256635525, 'worse'),\n",
       " (-14.226544332275056, 'terrible'),\n",
       " (-14.197902733280756, 'dull'),\n",
       " (13.881969626133381, '810'),\n",
       " (-13.855992129665413, 'disappointment'),\n",
       " (-13.708671589825316, 'forgettable')]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "term_coef = [(coef[word_dict[w]], w) for w in word_dict]\n",
    "term_coef = sorted(term_coef, key=lambda pair: abs(pair[0]), reverse=True)\n",
    "print(\"Need sõnad mõjutavad arvustuse sentimenti enim (koefitsent, sõna):\\n\")\n",
    "term_coef[:20]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
